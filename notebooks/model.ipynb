{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07cc76e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow (CPU) into the current kernel\n",
    "# Using the CPU build is usually smaller and more compatible for notebook runs.\n",
    "%pip install tensorflow-cpu --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4cbbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install runtime dependencies into the current kernel\n",
    "# This ensures required packages are available in whichever kernel is active.\n",
    "%pip install scikit-learn --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead78909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "numpy 1.26.4 pandas 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Import TensorFlow if available; otherwise continue and set USE_TF flag\n",
    "USE_TF = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    USE_TF = True\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "except Exception as e:\n",
    "    tf = None\n",
    "    keras = None\n",
    "    layers = None\n",
    "    print(\"TensorFlow not available; TF-dependent cells will be skipped. Error:\", e)\n",
    "\n",
    "# Convenience\n",
    "print(\"numpy\", np.__version__, \"pandas\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9641d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "if USE_TF:\n",
    "    tf.random.set_seed(42)\n",
    "else:\n",
    "    # fallback to numpy seed when TensorFlow isn't available\n",
    "    np.random.seed(42)\n",
    "    print(\"Using numpy RNG seed since TensorFlow is unavailable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539ebf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joblib version: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "print('joblib version:', joblib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14c9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 1) Synthetic dataset creation\n",
    "# ---------------------------\n",
    "def generate_synthetic_dataset(n_samples=20000, n_users=200):\n",
    "    # Network-like features (inspired by KDD features)\n",
    "    duration = np.random.exponential(scale=1.0, size=n_samples) * 10  # seconds\n",
    "    src_bytes = np.random.exponential(scale=300, size=n_samples)\n",
    "    dst_bytes = np.random.exponential(scale=300, size=n_samples)\n",
    "    wrong_fragment = np.random.poisson(0.01, n_samples)\n",
    "    urgent = np.random.poisson(0.005, n_samples)\n",
    "    count_same_dst = np.random.poisson(2, n_samples)\n",
    "    srv_count = np.random.poisson(5, n_samples)\n",
    "    protocol_type = np.random.choice([0,1,2], size=n_samples)  # tcp/udp/icmp encoded as ints\n",
    "    service = np.random.choice(range(10), size=n_samples)  # categorical service id\n",
    "    flag = np.random.choice(range(6), size=n_samples)  # flag id\n",
    "    \n",
    "    # User behavior features (per event map to a user)\n",
    "    user_ids = np.random.choice([f\"user_{i}\" for i in range(n_users)], size=n_samples)\n",
    "    # create per-user baseline stats\n",
    "    user_baseline = {}\n",
    "    for i in range(n_users):\n",
    "        uid = f\"user_{i}\"\n",
    "        user_baseline[uid] = {\n",
    "            \"avg_login_hour\": np.random.uniform(8,18),  # typical active hours\n",
    "            \"device_count\": np.random.randint(1,5),\n",
    "            \"sensitive_access_rate\": np.random.beta(1.2, 8)  # probability of accessing sensitive resources\n",
    "        }\n",
    "    # For each event, produce user-behavior features (sometimes anomalous)\n",
    "    avg_login_hour = np.array([user_baseline[u][\"avg_login_hour\"] for u in user_ids])\n",
    "    device_count = np.array([user_baseline[u][\"device_count\"] for u in user_ids])\n",
    "    sensitive_access_prob = np.array([user_baseline[u][\"sensitive_access_rate\"] for u in user_ids])\n",
    "    \n",
    "    # Current login hour for the event (may deviate)\n",
    "    login_hour = (avg_login_hour + np.random.normal(scale=4.0, size=n_samples)) % 24\n",
    "    # new device flag: occurs rarely, more likely if device_count small\n",
    "    new_device_flag = (np.random.rand(n_samples) < 0.02).astype(int)\n",
    "    # user anomalous flag (simulated ground truth) depends on combination\n",
    "    # We'll create anomalies: large outbound bytes + login at odd hour + sensitive access\n",
    "    sensitive_access = (np.random.rand(n_samples) < sensitive_access_prob).astype(int)\n",
    "    \n",
    "    # Label generation (0 normal, 1 anomaly)\n",
    "    # Base anomaly probability\n",
    "    base_prob = 0.02\n",
    "    anomaly_score = (\n",
    "        (src_bytes > 1500).astype(int)*0.4 +\n",
    "        (dst_bytes > 1500).astype(int)*0.3 +\n",
    "        (np.abs(login_hour - avg_login_hour) > 6).astype(int)*0.2 +\n",
    "        new_device_flag*0.2 +\n",
    "        sensitive_access*0.2 +\n",
    "        (count_same_dst > 10).astype(int)*0.2\n",
    "    )\n",
    "    prob = base_prob + 0.6 * (anomaly_score.clip(0,1))\n",
    "    labels = (np.random.rand(n_samples) < prob).astype(int)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"duration\": duration,\n",
    "        \"src_bytes\": src_bytes,\n",
    "        \"dst_bytes\": dst_bytes,\n",
    "        \"wrong_fragment\": wrong_fragment,\n",
    "        \"urgent\": urgent,\n",
    "        \"count_same_dst\": count_same_dst,\n",
    "        \"srv_count\": srv_count,\n",
    "        \"protocol_type\": protocol_type,\n",
    "        \"service\": service,\n",
    "        \"flag\": flag,\n",
    "        \"user_id\": user_ids,\n",
    "        \"avg_login_hour\": avg_login_hour,\n",
    "        \"login_hour\": login_hour,\n",
    "        \"device_count\": device_count,\n",
    "        \"new_device_flag\": new_device_flag,\n",
    "        \"sensitive_access\": sensitive_access,\n",
    "        \"label\": labels\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12dc0ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (12000, 17)\n"
     ]
    }
   ],
   "source": [
    "df = generate_synthetic_dataset(n_samples=12000, n_users=250)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "# Use ace_tools.display_dataframe_to_user if available (it will be used automatically by the environment)\n",
    "try:\n",
    "    from ace_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"sample_dataset\", df.head(200))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4b6894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backend/models/scaler_net.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 2) Feature engineering\n",
    "# ---------------------------\n",
    "# numeric features for autoencoder\n",
    "network_features = [\"duration\",\"src_bytes\",\"dst_bytes\",\"wrong_fragment\",\"urgent\",\"count_same_dst\",\"srv_count\"]\n",
    "cat_features = [\"protocol_type\",\"service\",\"flag\"]\n",
    "user_features = [\"login_hour\",\"avg_login_hour\",\"device_count\",\"new_device_flag\",\"sensitive_access\"]\n",
    "\n",
    "# One-hot encode small categorical features (protocol_type, flag) and service as embedding via one-hot (small)\n",
    "df_enc = pd.get_dummies(df, columns=[\"protocol_type\",\"flag\",\"service\"], prefix=[\"proto\",\"flag\",\"svc\"])\n",
    "feature_cols = network_features + [c for c in df_enc.columns if c.startswith((\"proto_\",\"flag_\",\"svc_\"))]  # network for autoencoder/encoder\n",
    "\n",
    "X_net = df_enc[feature_cols].values\n",
    "X_user = df[user_features].values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Standardize network features (important for autoencoder)\n",
    "scaler_net = StandardScaler()\n",
    "X_net_scaled = scaler_net.fit_transform(X_net)\n",
    "\n",
    "# Save scaler\n",
    "os.makedirs(\"backend/models\", exist_ok=True)\n",
    "joblib.dump(scaler_net, \"backend/models/scaler_net.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29de8a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoded_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m3,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoded_vector (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m520\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,232</span> (47.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,232\u001b[0m (47.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,232</span> (47.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,232\u001b[0m (47.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.9077 - val_loss: 0.7939\n",
      "Epoch 2/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.9077 - val_loss: 0.7939\n",
      "Epoch 2/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6888 - val_loss: 0.5853\n",
      "Epoch 3/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6888 - val_loss: 0.5853\n",
      "Epoch 3/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5110 - val_loss: 0.4498\n",
      "Epoch 4/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5110 - val_loss: 0.4498\n",
      "Epoch 4/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4003 - val_loss: 0.3676\n",
      "Epoch 5/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4003 - val_loss: 0.3676\n",
      "Epoch 5/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3393 - val_loss: 0.3238\n",
      "Epoch 6/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3393 - val_loss: 0.3238\n",
      "Epoch 6/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3019 - val_loss: 0.2924\n",
      "Epoch 7/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3019 - val_loss: 0.2924\n",
      "Epoch 7/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2736 - val_loss: 0.2693\n",
      "Epoch 8/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2736 - val_loss: 0.2693\n",
      "Epoch 8/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2521 - val_loss: 0.2516\n",
      "Epoch 9/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2521 - val_loss: 0.2516\n",
      "Epoch 9/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2363 - val_loss: 0.2393\n",
      "Epoch 10/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2363 - val_loss: 0.2393\n",
      "Epoch 10/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2247 - val_loss: 0.2297\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2247 - val_loss: 0.2297\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Encoded feature shape: (12000, 8)\n",
      "Encoded feature shape: (12000, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 3) Non-symmetric deep autoencoder (encoder-only)\n",
    "# ---------------------------\n",
    "input_dim = X_net_scaled.shape[1]\n",
    "encoding_dim = max(8, input_dim // 3)\n",
    "\n",
    "# Build a simple encoder (non-symmetric: no decoder for fast encoding)\n",
    "encoder_inputs = keras.Input(shape=(input_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(128, activation=\"relu\")(encoder_inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "encoded = layers.Dense(encoding_dim, activation=\"relu\", name=\"encoded_vector\")(x)\n",
    "encoder = keras.Model(encoder_inputs, encoded, name=\"encoder_model\")\n",
    "encoder.summary()\n",
    "\n",
    "# Train a full autoencoder that has a decoder to allow reconstruction loss (but we will keep encoder)\n",
    "# Full autoencoder for training stability\n",
    "ae_input = keras.Input(shape=(input_dim,), name=\"ae_input\")\n",
    "ae_x = layers.Dense(128, activation=\"relu\")(ae_input)\n",
    "ae_x = layers.Dense(64, activation=\"relu\")(ae_x)\n",
    "ae_encoded = layers.Dense(encoding_dim, activation=\"relu\")(ae_x)\n",
    "ae_x = layers.Dense(64, activation=\"relu\")(ae_encoded)\n",
    "ae_x = layers.Dense(128, activation=\"relu\")(ae_x)\n",
    "ae_decoded = layers.Dense(input_dim, activation=\"linear\")(ae_x)\n",
    "autoencoder = keras.Model(ae_input, ae_decoded, name=\"autoencoder_full\")\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train autoencoder quickly (small epochs for demo)\n",
    "history = autoencoder.fit(X_net_scaled, X_net_scaled, epochs=10, batch_size=256, validation_split=0.1, verbose=1)\n",
    "# After training, extract encoder by copying layers\n",
    "# Build encoder from trained autoencoder\n",
    "encoder = keras.Model(autoencoder.input, autoencoder.layers[3].output)  # depends on layer indices above\n",
    "encoded_X = encoder.predict(X_net_scaled, batch_size=512)\n",
    "\n",
    "# Save encoder\n",
    "encoder.save(\"backend/models/encoder_model.keras\", include_optimizer=False)\n",
    "\n",
    "print(\"Encoded feature shape:\", encoded_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d6f3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report (RandomForest on encoded+user features):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9433    1.0000    0.9708      2830\n",
      "           1     0.0000    0.0000    0.0000       170\n",
      "\n",
      "    accuracy                         0.9433      3000\n",
      "   macro avg     0.4717    0.5000    0.4854      3000\n",
      "weighted avg     0.8899    0.9433    0.9158      3000\n",
      "\n",
      "ROC-AUC: 0.7341010184992726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Duk\\S3\\AI Pro\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "e:\\Duk\\S3\\AI Pro\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "e:\\Duk\\S3\\AI Pro\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 4) Train RandomForest on encoded network features + user features\n",
    "# ---------------------------\n",
    "# Normalize/scale user features\n",
    "scaler_user = StandardScaler()\n",
    "X_user_scaled = scaler_user.fit_transform(X_user)\n",
    "joblib.dump(scaler_user, \"backend/models/scaler_user.joblib\")\n",
    "\n",
    "# Combined feature vector\n",
    "X_combined = np.hstack([encoded_X, X_user_scaled])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save classifier\n",
    "joblib.dump(clf, \"backend/models/rf_ueba_net.joblib\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(\"\\nClassification report (RandomForest on encoded+user features):\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "try:\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(\"ROC-AUC:\", auc)\n",
    "except Exception as e:\n",
    "    print(\"AUC error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4777ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m stream_sample\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     38\u001b[0m     res \u001b[38;5;241m=\u001b[39m score_event(row)\n\u001b[0;32m     39\u001b[0m     out \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: i,\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m: res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_contributors\u001b[39m\u001b[38;5;124m\"\u001b[39m: res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_contributors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39misoformat() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     }\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     48\u001b[0m         alerts\u001b[38;5;241m.\u001b[39mappend(out)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 5) Simulated streaming scoring + alerting\n",
    "# ---------------------------\n",
    "# Function to score an event (single row) using stored models: scaler_net, encoder, scaler_user, clf\n",
    "def load_models_from_disk():\n",
    "    scaler_net = joblib.load(\"backend/models/scaler_net.joblib\")\n",
    "    scaler_user = joblib.load(\"backend/models/scaler_user.joblib\")\n",
    "    clf = joblib.load(\"backend/models/rf_ueba_net.joblib\")\n",
    "    encoder = tf.keras.models.load_model(\"backend/models/encoder_model.keras\", compile=False)\n",
    "    return scaler_net, scaler_user, encoder, clf\n",
    "\n",
    "scaler_net_, scaler_user_, encoder_, clf_ = load_models_from_disk()\n",
    "\n",
    "def score_event(event_row):\n",
    "    # event_row: pd.Series containing raw event fields (must match df columns)\n",
    "    # preprocess network features similarly\n",
    "    net = event_row[feature_cols].values.reshape(1,-1)\n",
    "    net_scaled = scaler_net_.transform(net)\n",
    "    encoded = encoder_.predict(net_scaled)\n",
    "    user = event_row[user_features].values.reshape(1,-1)\n",
    "    user_scaled = scaler_user_.transform(user)\n",
    "    combined = np.hstack([encoded, user_scaled])\n",
    "    proba = clf_.predict_proba(combined)[0,1]\n",
    "    pred = int(proba > 0.5)\n",
    "    # simple explainability: contributions via tree feature importances (mapped back approximately)\n",
    "    # We compute SHAP-like approx by multiplying features by feature_importances where possible\n",
    "    importances = clf_.feature_importances_\n",
    "    contrib = combined.flatten() * importances[:combined.shape[1]]\n",
    "    top_idx = np.argsort(-np.abs(contrib))[:5]\n",
    "    top_feats = [{\"feature_index\": int(i), \"value\": float(combined.flatten()[i]), \"importance\": float(importances[i]), \"score_contrib\": float(contrib[i])} for i in top_idx]\n",
    "    return {\"score\": float(proba), \"alert\": bool(pred), \"top_contributors\": top_feats}\n",
    "\n",
    "\n",
    "# Simulate streaming: pick random events and score them\n",
    "stream_sample = df_enc.sample(20, random_state=1).reset_index(drop=True)\n",
    "alerts = []\n",
    "for i, row in stream_sample.iterrows():\n",
    "    res = score_event(row)\n",
    "    out = {\n",
    "        \"event_index\": i,\n",
    "        \"user_id\": row[\"user_id\"],\n",
    "        \"score\": res[\"score\"],\n",
    "        \"alert\": res[\"alert\"],\n",
    "        \"top_contributors\": res[\"top_contributors\"],\n",
    "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "    }\n",
    "    if res[\"alert\"]:\n",
    "        alerts.append(out)\n",
    "\n",
    "print(f\"\\nSimulated stream produced {len(alerts)} alerts out of {len(stream_sample)} events.\")\n",
    "if alerts:\n",
    "    alerts_df = pd.DataFrame(alerts)\n",
    "    try:\n",
    "        display_dataframe_to_user(\"simulated_alerts\", alerts_df)\n",
    "    except Exception:\n",
    "        print(alerts_df.to_dict(orient='records'))\n",
    "\n",
    "# Save a small metadata file describing model and how to replace dataset\n",
    "meta = {\n",
    "    \"note\": \"Prototype models saved. Replace synthetic dataset with NSL-KDD/real logs and retrain for production.\",\n",
    "    \"models\": {\n",
    "        \"encoder\": \"backend/models/encoder_model.keras\",\n",
    "        \"rf\": \"backend/models/rf_ueba_net.joblib\",\n",
    "        \"scaler_net\": \"backend/models/scaler_net.joblib\",\n",
    "        \"scaler_user\": \"backend/models/scaler_user.joblib\"\n",
    "    },\n",
    "    \"feature_columns_network\": feature_cols,\n",
    "    \"feature_columns_user\": user_features\n",
    "}\n",
    "with open(\"backend/models/README_model_metadata.json\",\"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"\\nModels and metadata saved to backend/models/. You can download them from the UI.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
